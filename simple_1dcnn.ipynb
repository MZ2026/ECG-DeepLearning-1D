{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5105a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ast, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import ptbxl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee45de34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random seed set to 42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility across Python, NumPy, and PyTorch\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\" Random seed set to {seed}\\n\")\n",
    "\n",
    "set_seed(42)\n",
    "SEED = 42  # Keep SEED defined for consistency (used later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f35c8a",
   "metadata": {},
   "source": [
    "### Dataset Setup and Initialization\n",
    "Load the PTB-XL metadata, select training, validation, and test folds, set the ECG sampling  \n",
    "frequency (100Hz or 500Hz), and choose between filtered (AFIB vs NORM) or unfiltered   \n",
    "(AFIB vs ALL other classes) dataset modes. The PTB-XL dataset is pre-divided into 10  \n",
    " stratified folds (1–10).One or multiple folds can be selected for each split — for example:    \n",
    "rain_folds = [1, 2, 3]  \n",
    "val_folds = [9]  \n",
    "test_folds = [10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43e7a5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected folds:\n",
      " Train: Fold [1, 2] (4356 samples)\n",
      " Validation: Fold [9] (2183 samples)\n",
      " Test: Fold [10] (2198 samples)\n",
      " Total: 8737 samples\n",
      "\n",
      "Using ECG signal (100Hz)\n",
      "\n",
      "Using full dataset (AFIB vs ALL — all non-AFIB treated as NORM)\n",
      "Sampling rate: 100Hz\n",
      "Train      | AFIB: 302    | Non-AFIB: 4054   | Total: 4356\n",
      "Validation | AFIB: 151    | Non-AFIB: 2032   | Total: 2183\n",
      "Test       | AFIB: 152    | Non-AFIB: 2046   | Total: 2198\n",
      "\n",
      "Totals:\n",
      " AFIB: 605 | Non-AFIB: 8132 | Total: 8737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"ptb-xl-dataset-1.0.3\"\n",
    "csv_path = os.path.join(data_path, \"ptbxl_database.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Run dataset setup\n",
    "train_data, val_data, test_data, freq = ptbxl_utils.setup_dataset(\n",
    "    df,\n",
    "    train_folds=[1,2],\n",
    "    val_folds=[9],          \n",
    "    test_folds=[10],        \n",
    "    use_500Hz= False,        # True: use 500Hz ECG signals  |  False: 100Hz\n",
    "    use_filtered=False       # True: Filterd data(only AFIB and Norm) | False: Full(AFIB vs all others)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aebfdd",
   "metadata": {},
   "source": [
    "#### Create PyTorch Datasets class and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b6626f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing path: records100/00000/00002_lr\n",
      "Loaded shape: (1000, 12)\n"
     ]
    }
   ],
   "source": [
    "# Load an ECG record from PTB-XL using WFDB.\n",
    "def load_ecg(record_path: str) -> np.ndarray:\n",
    "    import os\n",
    "\n",
    "    # Automatically prepend the dataset folder if missing\n",
    "    base_dir = os.path.join(os.getcwd(), \"ptb-xl-dataset-1.0.3\")\n",
    "    full_path = os.path.join(base_dir, record_path)\n",
    "\n",
    "    if not os.path.exists(full_path + \".hea\"):\n",
    "        raise FileNotFoundError(f\"ECG file not found at {full_path}.hea\")\n",
    "\n",
    "    record = wfdb.rdrecord(full_path)\n",
    "    return record.p_signal.astype(np.float32)\n",
    "\n",
    "# Quick test\n",
    "sample_path = train_data.iloc[0][\"filename\"]\n",
    "print(\"Testing path:\", sample_path)\n",
    "print(\"Loaded shape:\", load_ecg(sample_path).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c83e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. PyTorch Dataset class\n",
    "class PTBXL_Dataset(Dataset):\n",
    "    \"\"\"Custom dataset: loads ECG signals, normalizes per-lead, returns (tensor, label).\"\"\"\n",
    "    def __init__(self, df_split, mean=None, std=None):\n",
    "        self.paths = df_split[\"filename\"].values\n",
    "        self.labels = df_split[\"label\"].astype(np.float32).values\n",
    "\n",
    "        if mean is None or std is None:\n",
    "            samp = df_split.sample(min(100, len(df_split)), random_state=SEED)[\"filename\"]\n",
    "            cat = np.concatenate([load_ecg(p) for p in samp], axis=0)\n",
    "            self.mean = cat.mean(axis=0)\n",
    "            self.std  = cat.std(axis=0) + 1e-7\n",
    "        else:\n",
    "            self.mean, self.std = mean, std\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        arr = load_ecg(self.paths[idx])\n",
    "        arr = (arr - self.mean) / self.std\n",
    "        arr = torch.tensor(arr.T, dtype=torch.float32)  # (12, L)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return arr, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22378d",
   "metadata": {},
   "source": [
    "DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d647652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready (GPU = False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "train_set = PTBXL_Dataset(train_data)\n",
    "val_set   = PTBXL_Dataset(val_data,  mean=train_set.mean, std=train_set.std)\n",
    "test_set  = PTBXL_Dataset(test_data, mean=train_set.mean, std=train_set.std)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=0, pin_memory=use_cuda)\n",
    "val_loader   = DataLoader(val_set,   batch_size=16, shuffle=False, num_workers=0, pin_memory=use_cuda)\n",
    "test_loader  = DataLoader(test_set,  batch_size=16, shuffle=False, num_workers=0, pin_memory=use_cuda)\n",
    "\n",
    "print(f\"DataLoaders ready (GPU = {use_cuda})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012490bd",
   "metadata": {},
   "source": [
    "### Define a simple 1D-CNN model(3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c930b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected flatten dimension: 16000 for input length 1000\n",
      "Model created for input length 1000, running on cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CNN model (auto-adapts to input length)\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_length, in_channels=12):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional feature extractor\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 32, 5, padding=2), nn.ReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool1d(2),\n",
    "        )\n",
    "\n",
    "        # Dynamically compute flatten dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, input_length)\n",
    "            conv_out = self.conv_layers(dummy)\n",
    "            flat_dim = conv_out.shape[1] * conv_out.shape[2]\n",
    "        print(f\"Detected flatten dimension: {flat_dim} for input length {input_length}\")\n",
    "\n",
    "        #Fully connected classification head\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "\n",
    "\n",
    "# pick correct input length automatically\n",
    "INPUT_LEN = 5000 if freq == 500 else 1000\n",
    "model = CNN1D(input_length=INPUT_LEN)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(f\"Model created for input length {INPUT_LEN}, running on {device}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8b2e0",
   "metadata": {},
   "source": [
    "##### Define training and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71eb8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for one epoch.\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train(); total_loss = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y.unsqueeze(1))  # binary case\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "# Evaluate the model on validation/test data.\n",
    "def evaluate_model(model, loader, criterion, device, threshold=0.5):\n",
    "    model.eval(); total_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y.unsqueeze(1))\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            probs = torch.sigmoid(out)\n",
    "            preds = (probs > threshold).squeeze(1).long()\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss, f1, acc, np.array(y_true), np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f2c0c9",
   "metadata": {},
   "source": [
    "#### Implement the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1fdbe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  Train Loss 0.2471 | Val Loss 0.1881 | Val F1 0.0000 | Val Acc 0.9308\n",
      "Epoch 2/10\n",
      "  Train Loss 0.1644 | Val Loss 0.1314 | Val F1 0.0000 | Val Acc 0.9308\n",
      "Epoch 3/10\n",
      "  Train Loss 0.1107 | Val Loss 0.1218 | Val F1 0.0000 | Val Acc 0.9295\n",
      "Epoch 4/10\n",
      "  Train Loss 0.0844 | Val Loss 0.1256 | Val F1 0.5809 | Val Acc 0.9537\n",
      "  New model saved\n",
      "\n",
      "Epoch 5/10\n",
      "  Train Loss 0.0595 | Val Loss 0.1324 | Val F1 0.7081 | Val Acc 0.9569\n",
      "  New model saved\n",
      "\n",
      "Epoch 6/10\n",
      "  Train Loss 0.0489 | Val Loss 0.1258 | Val F1 0.6710 | Val Acc 0.9533\n",
      "Epoch 7/10\n",
      "  Train Loss 0.0442 | Val Loss 0.1637 | Val F1 0.6769 | Val Acc 0.9615\n",
      "Epoch 8/10\n",
      "  Train Loss 0.0176 | Val Loss 0.1855 | Val F1 0.7039 | Val Acc 0.9588\n",
      "Epoch 9/10\n",
      "  Train Loss 0.0213 | Val Loss 0.3012 | Val F1 0.6295 | Val Acc 0.9574\n",
      "Epoch 10/10\n",
      "  Train Loss 0.0229 | Val Loss 0.2068 | Val F1 0.6622 | Val Acc 0.9537\n",
      "Best model (highest Val F1) re-loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11. Training loop\n",
    "start_time = time.time()\n",
    "NUM_EPOCHS = 10\n",
    "best_f1 = 0.0; best_state = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    tr_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_f1, val_acc, y_true, y_pred = evaluate_model(model, val_loader, criterion, device)\n",
    "    print(f\"  Train Loss {tr_loss:.4f} | Val Loss {val_loss:.4f} | Val F1 {val_f1:.4f} | Val Acc {val_acc:.4f}\")\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1; best_state = model.state_dict()\n",
    "        print(\"  New model saved\\n\")\n",
    "\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(\"Best model (highest Val F1) re-loaded.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f40b33",
   "metadata": {},
   "source": [
    "#### Evaluate the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6afedf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Metrics Summary\n",
      "  Test Loss: 0.2278\n",
      "  Test F1-score: 0.6148\n",
      "  Test Accuracy: 0.9527\n",
      "  Sampling frequency: 100Hz\n",
      "  Device used: CPU\n",
      "\n",
      "Label mapping:\n",
      "0: Normal Sinus Rhythm (NORM)\n",
      "1: Atrial Fibrillation (AFIB)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.97      0.98      0.97      2046\n",
      "        AFIB       0.70      0.55      0.61       152\n",
      "\n",
      "    accuracy                           0.95      2198\n",
      "   macro avg       0.84      0.76      0.79      2198\n",
      "weighted avg       0.95      0.95      0.95      2198\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2011   35]\n",
      " [  69   83]]\n",
      "2011 Normal ECGs correctly classified (98.3%)\n",
      "35 Normal ECGs wrongly predicted as AFIB (1.7%) [False Positives]\n",
      "83 AFIB ECGs correctly classified (54.6%)\n",
      "69 AFIB ECGs wrongly predicted as Normal (45.4%) [False Negatives]\n",
      "\n",
      "Dataset Sizes:\n",
      "Training records:   4356\n",
      "Validation records: 2183\n",
      "Test records:       2198\n",
      "\n",
      "Runtime: 00h 04m 58s\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the Test Set\n",
    "test_loss, test_f1, test_acc, y_true, y_pred = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\nTest Metrics Summary\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test F1-score: {test_f1:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Sampling frequency: {freq}Hz\")\n",
    "print(f\"  Device used: {'GPU' if device.type == 'cuda' else 'CPU'}\")\n",
    "\n",
    "\n",
    "# Switch model to evaluation mode for consistent results\n",
    "model.eval()\n",
    "\n",
    "# Label mapping for clarity\n",
    "print(\"\\nLabel mapping:\")\n",
    "print(\"0: Normal Sinus Rhythm (NORM)\")\n",
    "print(\"1: Atrial Fibrillation (AFIB)\")\n",
    "\n",
    "# Detailed test report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"AFIB\"]))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "total_normal = cm[0].sum()\n",
    "total_afib = cm[1].sum()\n",
    "\n",
    "print(f\"{tn} Normal ECGs correctly classified ({tn/total_normal*100:.1f}%)\")\n",
    "print(f\"{fp} Normal ECGs wrongly predicted as AFIB ({fp/total_normal*100:.1f}%) [False Positives]\")\n",
    "print(f\"{tp} AFIB ECGs correctly classified ({tp/total_afib*100:.1f}%)\")\n",
    "print(f\"{fn} AFIB ECGs wrongly predicted as Normal ({fn/total_afib*100:.1f}%) [False Negatives]\")\n",
    "\n",
    "# Dataset Summary\n",
    "print(\"\\nDataset Sizes:\")\n",
    "print(f\"Training records:   {len(train_data)}\")\n",
    "print(f\"Validation records: {len(val_data)}\")\n",
    "print(f\"Test records:       {len(test_data)}\")\n",
    "\n",
    "# Runtime Summary\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "hours, rem = divmod(total_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(f\"\\nRuntime: {int(hours):02d}h {int(minutes):02d}m {int(seconds):02d}s\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
